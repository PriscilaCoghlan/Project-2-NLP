{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Import Libraries:\n",
    "\n",
    "# Data Manipulation:\n",
    "# pandas: Provides data structures like DataFrames, which are useful for handling and processing structured data.\n",
    "    \n",
    "import pandas as pd\n",
    "    \n",
    "# Feature Extraction:\n",
    "# CountVectorizer: Converts a collection of text documents to a matrix of token counts.\n",
    "# TfidfVectorizer: Converts a collection of raw documents to a matrix of TF-IDF features, which reflect the importance of words in the documents.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model Training:\n",
    "# MultinomialNB: Implements the Multinomial Naive Bayes algorithm, which is suitable for classification with discrete features (like word counts for text classification).\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Model Evaluation:\n",
    "# accuracy_score: Calculates the accuracy of the model by comparing the predicted labels with the true labels.\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a - Define the file path once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"D:/OneDrive - Royal HaskoningDHV/920791/Pri 3/ironhack/nlp-project/Project-2-NLP/dataset/training_data_lowercase.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b- Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2c- Install the chardet library detect the encoding programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in c:\\users\\920791\\appdata\\local\\miniconda3\\lib\\site-packages (5.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install chardet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2d- Detect the encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The detected encoding is: UTF-8-SIG\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "# Read the first few bytes of the file to detect the encoding\n",
    "with open(file_path, 'rb') as file:\n",
    "    raw_data = file.read(10000)\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding']\n",
    "    print(f\"The detected encoding is: {encoding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2e- Load the dataset with the correct encoding to handle BOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2f- Display the column names to verify they are correctly parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['0\\tdonald trump sends out embarrassing new year‚s eve message; this is disturbing'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2g- Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0\\tdonald trump sends out embarrassing new year‚s eve message; this is disturbing\n",
      "0  0\\tdrunk bragging trump staffer started russia...                               \n",
      "1  0\\tsheriff david clarke becomes an internet jo...                               \n",
      "2  0\\ttrump is so obsessed he even has obama‚s na...                               \n",
      "3  0\\tpope francis just called out donald trump d...                               \n",
      "4  0\\tracist alabama cops brutalize black boy whi...                               \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2h- Load the dataset with the correct encoding to handle BOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path, encoding='utf-8-sig', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2i- Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  0\\tdonald trump sends out embarrassing new yea...\n",
      "1  0\\tdrunk bragging trump staffer started russia...\n",
      "2  0\\tsheriff david clarke becomes an internet jo...\n",
      "3  0\\ttrump is so obsessed he even has obama‚s na...\n",
      "4  0\\tpope francis just called out donald trump d...\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2j- Separate the first part of the sentences with the separator '0\\t' and assign it as the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['label', 'text']] = data[0].str.split('\\t', n=1, expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the original column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2k- Display the first few rows of the dataset after removing the first part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0     0  donald trump sends out embarrassing new year‚s...\n",
      "1     0  drunk bragging trump staffer started russian c...\n",
      "2     0  sheriff david clarke becomes an internet joke ...\n",
      "3     0  trump is so obsessed he even has obama‚s name ...\n",
      "4     0  pope francis just called out donald trump duri...\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    0\n",
      "text     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 3a- Check for missing values in the dataset\n",
    "\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output 0    0: Means that the column 0 (the only column in your DataFrame) has zero missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in the 'label' column\n",
    "\n",
    "data = data.dropna(subset=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    0\n",
      "text     0\n",
      "dtype: int64\n",
      "  label                                               text\n",
      "0     0  donald trump sends out embarrassing new year‚s...\n",
      "1     0  drunk bragging trump staffer started russian c...\n",
      "2     0  sheriff david clarke becomes an internet joke ...\n",
      "3     0  trump is so obsessed he even has obama‚s name ...\n",
      "4     0  pope francis just called out donald trump duri...\n",
      "(34152, 2)\n"
     ]
    }
   ],
   "source": [
    "# 3b- Check Data After Preprocessing\n",
    "\n",
    "print(data.isnull().sum())\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4- Split Data: We split the dataset into training and testing sets. The training set is used to train the model, and the testing set is used to evaluate the model’s performance.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5- Feature Extraction: We use two methods to convert the text data into numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5a- Method 1: Count Vectorizer: Converts text into a matrix of token counts.\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5b- Method 2: TF-IDF Vectorizer: Converts text into a matrix of TF-IDF features, which reflect the importance of words in the documents.\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6- Train and Evaluate Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6a- Using Count Vectorizer features\n",
    "model_count = MultinomialNB()\n",
    "model_count.fit(X_train_count, y_train)\n",
    "y_pred_count = model_count.predict(X_test_count)\n",
    "accuracy_count = accuracy_score(y_test, y_pred_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b- Using TF-IDF Vectorizer features\n",
    "model_tfidf = MultinomialNB()\n",
    "model_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best feature representation is Count Vectorizer with an accuracy of 0.9448104230712926.\n"
     ]
    }
   ],
   "source": [
    "# 6c- Compare the accuracies\n",
    "if accuracy_tfidf > accuracy_count:\n",
    "    best_representation = \"TF-IDF Vectorizer\"\n",
    "    best_accuracy = accuracy_tfidf\n",
    "else:\n",
    "    best_representation = \"Count Vectorizer\"\n",
    "    best_accuracy = accuracy_count\n",
    "\n",
    "print(f\"The best feature representation is {best_representation} with an accuracy of {best_accuracy}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
